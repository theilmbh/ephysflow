{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer: passaro\n"
     ]
    }
   ],
   "source": [
    "# tampering with the kwd file on passaro\n",
    "# the /experiment folder is on the ssd of passaro (/usr/local/experiment/raw_data)\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pdb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.signal as sg\n",
    "import math\n",
    "import scipy as sp\n",
    "import socket\n",
    "import os\n",
    "import wave\n",
    "import struct\n",
    "import h5py\n",
    "from scipy.signal import hilbert\n",
    "from basic_viewing import h5_functions as h5\n",
    "import glob\n",
    "import errno    \n",
    "import os\n",
    "import shutil as sh\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Check wihic computer to decide where the things are mounted\n",
    "comp_name=socket.gethostname()\n",
    "print 'Computer: ' + comp_name\n",
    "if  comp_name == 'chim':\n",
    "    #sys.path.append('/Users/zeke/experiment/ephysDataManagement/ephysScripts')\n",
    "    experiment_folder = os.path.join(os.path.abspath('W:') , \"earneodo\", \"bci_zf\")\n",
    "elif  'lookfar' in comp_name:\n",
    "    repos_folder = os.path.abspath('/Users/zeke/reposs')\n",
    "    experiment_folder = os.path.join(os.path.abspath('/Volumes'), \n",
    "                                     'gentner', \n",
    "                                     'earneodo', \n",
    "                                     'bci_zf')\n",
    "\n",
    "elif 'lintu' in comp_name:\n",
    "    repos_folder = os.path.abspath('/mnt/cube/earneodo/repos')\n",
    "    experiment_folder = os.path.join('/mnt/cube/earneodo/bci_zf/')\n",
    "\n",
    "elif 'passaro' in comp_name:\n",
    "    repos_folder = os.path.abspath('/mnt/cube/earneodo/repos')\n",
    "    cube_experiment_folder = os.path.join('/mnt/cube/earneodo/bci_zf/')\n",
    "    store_experiment_folder = os.path.join('/Data/bci_zf/')\n",
    "    experiment_folder = os.path.join('/usr/local/experiment')\n",
    "\n",
    "sys.path.append(os.path.join(repos_folder, 'soundflow', 'sound_tools'))\n",
    "sys.path.append(os.path.join(repos_folder, 'ephysflow'))\n",
    "sys.path.append(os.path.join(repos_folder, 'analysis-tools'))\n",
    "\n",
    "import soundtools as st\n",
    "import Kwik as oe\n",
    "from scipy.io import wavfile\n",
    "from file_tools import experiment as et\n",
    "\n",
    "def create_neural_data_set(data_set, parent_group, channel_list, \n",
    "                           frame_size=None, \n",
    "                           processing=None,\n",
    "                           *args, **kwargs):\n",
    "    # make the new dataset\n",
    "    d_type = data_set.dtype\n",
    "    nd_cols = np.array(channel_list).size\n",
    "    nd_rows = data_set.shape[0]\n",
    "    d_chunks = np.array(data_set.chunks)\n",
    "    \n",
    "    if d_chunks[0] > nd_rows:\n",
    "        d_chunks[0] = nd_rows\n",
    "    \n",
    "    if frame_size is None:\n",
    "        frame_size = d_chunks[0] if d_chunks[0]<nd_rows else nd_rows\n",
    "    \n",
    "    neural_dset = parent_group.create_dataset(\"data\", (nd_rows, nd_cols), \n",
    "                                              chunks=(d_chunks[0], nd_cols), dtype=d_type)\n",
    "    \n",
    "    copy_frame = np.zeros([frame_size, nd_cols], dtype=d_type)\n",
    "    n_full_frames = int(data_set.shape[0]/frame_size)\n",
    "    #print neural_dset.shape\n",
    "    # Fill the dataset:\n",
    "    for i in range(n_full_frames):\n",
    "        copy_frame = h5.load_table_slice(data_set, \n",
    "                                         np.arange(i*frame_size, (i+1)*frame_size), \n",
    "                                         channel_list)\n",
    "        neural_dset[i*frame_size: (i+1)*frame_size, 0:nd_cols] = copy_frame\n",
    "    \n",
    "    if frame_size*n_full_frames < nd_rows:\n",
    "        last_frame_size = nd_rows - frame_size*n_full_frames\n",
    "        copy_frame = np.zeros([last_frame_size, nd_cols], dtype=d_type)\n",
    "        copy_frame = h5.load_table_slice(data_set, \n",
    "                                         np.arange(n_full_frames*frame_size, \n",
    "                                                   n_full_frames*frame_size + last_frame_size),\n",
    "                                         channel_list)\n",
    "    # copy the attributes:\n",
    "    neural_dset.attrs.create('valid_samples', \n",
    "                             np.array(data_set.attrs['valid_samples'][channel_list]))\n",
    "    \n",
    "def copy_attribs(source, dest):\n",
    "    for key, attrib in source.attrs.iteritems():\n",
    "        dest.attrs.create(key, attrib, dtype=attrib.dtype)\n",
    "        \n",
    "def copy_application_data(source_rec, dest_rec, chan_list,\n",
    "                          resize_keys = ['channel_bit_volts', 'channel_sample_rates']):\n",
    "    \n",
    "    dest_rec.create_group('application_data')\n",
    "    for key, attrib in source_rec['application_data'].attrs.iteritems():\n",
    "        if not key in resize_keys:\n",
    "            dest_rec['application_data'].attrs.create(key, attrib, dtype=attrib.dtype)\n",
    "        else:\n",
    "            #print key\n",
    "            #print attrib[chan_list]\n",
    "            dest_rec['application_data'].attrs.create(key, attrib[chan_list], dtype=attrib.dtype)\n",
    "\n",
    "def check_rec_data(source_rec):\n",
    "    #check the data table of a rec group\n",
    "    data_set = source_rec['data']\n",
    "    valid_samples = None\n",
    "    try:\n",
    "        valid_samples = data_set.attrs.get('valid_samples')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    has_data = True if valid_samples is not None and valid_samples[0]>0 else False\n",
    "    return has_data\n",
    "\n",
    "def get_valid_samples(source_rec):\n",
    "    #get the valid samples declared by the data table attributes\n",
    "    data_set = source_rec['data']\n",
    "    valid_samples = None\n",
    "    try:\n",
    "        valid_samples = data_set.attrs.get('valid_samples')[0]\n",
    "    except:\n",
    "        pass\n",
    "    return valid_samples\n",
    "\n",
    "def create_data_groups(raw_file, new_file, chan_list):\n",
    "    for rec_name, rec_group in raw_file['/recordings'].iteritems():\n",
    "        new_file['/recordings'].create_group(rec_name)\n",
    "        copy_attribs(rec_group, new_file['/recordings'][rec_name])\n",
    "        copy_application_data(rec_group, new_file['/recordings'][rec_name], chan_list)\n",
    "        create_neural_data_set(rec_group['data'], new_file['/recordings'][rec_name], chan_list)\n",
    "        \n",
    "def make_shank_kwd(raw_file, out_file_path, chan_list):\n",
    "    ss_file = h5py.File(out_file_path, 'w')\n",
    "    copy_attribs(raw_file, ss_file)\n",
    "    ss_file.create_group('/recordings')\n",
    "    create_data_groups(raw_file, ss_file, chan_list) \n",
    "    ss_file.close()\n",
    "    \n",
    "# def list_flatten(x):\n",
    "#     return [val for sublist in x for val in sublist]\n",
    "\n",
    "def list_flatten(x):\n",
    "    result = []\n",
    "    for el in x:\n",
    "        if hasattr(el, \"__iter__\") and not isinstance(el, basestring):\n",
    "            result.extend(list_flatten(el))\n",
    "        else:\n",
    "            result.append(el)\n",
    "    return result\n",
    "        \n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def make_super_file(path):\n",
    "    new_file = h5py.File(path, 'w')\n",
    "    new_file.create_group('/recordings')\n",
    "    new_file.close()\n",
    "\n",
    "def save_ss_par(par, bird, sess):\n",
    "    fn = et.file_names(bird, sess)\n",
    "    par_path = et.file_path(fn, 'ss', 'par')\n",
    "    with open(par_path, 'w') as f:\n",
    "        written = yaml.dump(par, f)\n",
    "    return written\n",
    "\n",
    "def new_channel_config(chan_config):\n",
    "    new_chan_config = {}\n",
    "    chan_list = list_flatten([chans for chans in chan_config.itervalues()])\n",
    "    chan_list.sort()\n",
    "    raw_chans = np.array(chan_list) \n",
    "    for chan_group, channels in chan_config.iteritems():\n",
    "        if hasattr(channels, \"__iter__\") and not isinstance(channels, basestring):\n",
    "            new_chan_config[chan_group] = [int(np.where(raw_chans==r_ch)[0][0]) for r_ch in channels]\n",
    "        else:\n",
    "            new_chan_config[chan_group] = int(np.where(raw_chans==channels)[0][0])\n",
    "    return chan_list, new_chan_config\n",
    "    \n",
    "def create_data_group(raw_file, new_file, chan_list, new_rec_name):\n",
    "    for rec_name, rec_group in raw_file['/recordings'].iteritems():\n",
    "        new_file['/recordings'].create_group(rec_name)\n",
    "        copy_attribs(rec_group, new_file['/recordings'][rec_name])\n",
    "        copy_application_data(rec_group, new_file['/recordings'][rec_name], chan_list)\n",
    "        create_neural_data_set(rec_group['data'], new_file['/recordings'][rec_name], chan_list)\n",
    "\n",
    "def insert_neural_rec_group(dest_file, raw_rec_group, chan_list, new_group_name=None):\n",
    "    rec_name = raw_rec_group.name if new_group_name is None else new_group_name\n",
    "    dest_file['/recordings'].create_group(rec_name)\n",
    "    new_rec = dest_file['/recordings'][rec_name]\n",
    "    copy_application_data(raw_rec_group, new_rec, chan_list)\n",
    "    copy_attribs(raw_rec_group, new_rec)\n",
    "    create_neural_data_set(raw_rec_group['data'], new_rec, chan_list)\n",
    "    \n",
    "def modify_rec_group_attribs(kwd_file, rec_name, attr_dict, new_attr_dict = None):\n",
    "    for key, value in attr_dict.iteritems():\n",
    "        kwd_file['/recordings'][rec_name].attrs.modify(key, value)\n",
    "    \n",
    "    if new_attr_dict is not None:\n",
    "        for key, value in new_attr_dict.iteritems():\n",
    "            kwd_file['/recordings'][rec_name].attrs.create(key, value)\n",
    "\n",
    "def get_recs_list(experiment_file):\n",
    "    if experiment_file['/recordings'].keys() == []:\n",
    "        rec_list = None\n",
    "    else:\n",
    "        rec_list = np.array([int(i) for i in experiment_file['/recordings'].keys()])\n",
    "    return rec_list\n",
    "\n",
    "def get_experiment_endpoints(experiment_file):\n",
    "    recs_list = get_recs_list(experiment_file)\n",
    "    if recs_list is not None:\n",
    "        last_rec_num = np.max(recs_list)\n",
    "        last_rec_name = str(last_rec_num)\n",
    "        last_rec = experiment_file['/recordings'][last_rec_name]\n",
    "        last_rec_start = last_rec.attrs['start_sample']\n",
    "        last_rec_nsamples = last_rec['data'].shape[0]\n",
    "        next_rec_num = last_rec_num + 1\n",
    "        next_sample = last_rec_start + last_rec_nsamples\n",
    "    else:\n",
    "        next_rec_num = 0\n",
    "        next_sample = 0\n",
    "        \n",
    "    return [next_rec_num, next_sample]\n",
    "            \n",
    "\n",
    "def insert_experiment_groups(dest_file, raw_file, chan_list):\n",
    "    # all the recs in an experiment file if they pass check\n",
    "    for raw_rec_name, raw_rec_group in raw_file['/recordings'].iteritems():\n",
    "        #print 'rec {0}'.format(raw_rec_name)\n",
    "        s_f = raw_rec_group.attrs['sample_rate']\n",
    "        if check_rec_data(raw_rec_group):\n",
    "            target_rec_num, target_start_sample = get_experiment_endpoints(dest_file)\n",
    "            target_rec_name = str(target_rec_num)\n",
    "            target_start_time = int(target_start_sample/(0.001*s_f))\n",
    "            target_source = '{0}:''/''recordings/{1}'.format(raw_file.filename, raw_rec_name)\n",
    "            insert_neural_rec_group(dest_file, raw_rec_group, chan_list, new_group_name=target_rec_name)\n",
    "            modify_rec_group_attribs(dest_file, target_rec_name, \n",
    "                             {'start_sample' : target_start_sample},\n",
    "                              new_attr_dict = {'name' : target_source,\n",
    "                                               'start_time' : target_start_time})\n",
    "        else:\n",
    "            #print \"Skipping rec {0} with no data\".format(raw_rec_name)\n",
    "            pass\n",
    "\n",
    "def band_pass_filter_pars(s_f, exp_par):\n",
    "    filt_hi = exp_par['search_motiff']['filt_hi']\n",
    "    filt_lo = exp_par['search_motiff']['filt_lo']\n",
    "    hp_b, hp_a = sg.butter(4, filt_hi/(s_f/2.), btype='high')\n",
    "    lp_b, lp_a = sg.butter(4, filt_lo/(s_f/2.), btype='low')\n",
    "    return hp_b, hp_a, lp_b, lp_a\n",
    "\n",
    "def filter_bp(data, hp_b, hp_a, lp_b, lp_a):\n",
    "    x_hi = sg.filtfilt(hp_b, hp_a, data, axis=0)\n",
    "    x_lo = sg.filtfilt(lp_b, lp_a, x_hi, axis=0)\n",
    "    return x_lo\n",
    "\n",
    "def get_dset_group_attr(data_set, attr_name):\n",
    "    return data_set.parent.attrs[attr_name]\n",
    "\n",
    "def export_audio(data_set, chan_number, out_file_path, filter_func=None, args=(), **kwargs):\n",
    "    frame_size = 60 #seconds\n",
    "    s_f = get_dset_group_attr(data_set, 'sample_rate')\n",
    "    samples_frame = int(frame_size * s_f)\n",
    "    samples_data = data_set.shape[0]\n",
    "    \n",
    "    frame_buffer = np.zeros((samples_frame, 1), dtype = np.float)\n",
    "    frame_starts = np.arange(0, samples_data, samples_frame)\n",
    "    \n",
    "    wavefile = wave.open(out_file_path,'wb')\n",
    "    wavefile.setparams((1, 2, s_f, 0, 'NONE', 'not compressed'))\n",
    "    \n",
    "    for start in frame_starts:\n",
    "        end = min(start + samples_frame, samples_data)\n",
    "        frame_buffer[0: end-start, :] = h5.load_table_slice(data_set, \n",
    "                                                         np.arange(start, end),\n",
    "                                                         [chan_number])\n",
    "        frame_buffer[0:end-start, :] = filter_func(frame_buffer[0:end-start, :], \n",
    "                                                  *args, **kwargs)\n",
    "        wavefile.writeframes(frame_buffer[0: end-start].astype('h').tostring())\n",
    "    \n",
    "    wavefile.close()\n",
    "\n",
    "def extract_wav_chans(bird_id, sess, ch_name='mic'):\n",
    "    fn = et.file_names(bird_id, sess, 0)\n",
    "    super_file_path = os.path.join(fn['folders']['ss'], fn['files']['ss_raw'])\n",
    "    exp_par = et.get_parameters(bird_id, sess)\n",
    "    super_file = h5py.File(super_file_path, 'r')\n",
    "    rec_list = super_file['/recordings'].keys()\n",
    "    print rec_list\n",
    "\n",
    "    for rec, rec_grp in super_file['/recordings'].iteritems():\n",
    "        print rec\n",
    "        fn = et.file_names(bird_id, sess, int(rec))\n",
    "        data_set = rec_grp['data']\n",
    "        s_f = rec_grp.attrs['sample_rate']\n",
    "        wav_file_path = os.path.join(fn['folders']['ss'], fn['files'][ch_name])\n",
    "        print wav_file_path\n",
    "        export_audio(data_set, exp_par['channel_config'][ch_name], wav_file_path,\n",
    "                    filter_func=filter_bp,\n",
    "                    args=band_pass_filter_pars(s_f, exp_par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making supersession day-2016-09-14\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment.raw.kwd\n",
      "making the superfile\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_09-41-13_100/experiment1_100.raw.kwd\n",
      "2016-09-14_09-41-13_100\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_10-06-02_100/experiment1_100.raw.kwd\n",
      "2016-09-14_10-06-02_100\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_10-09-22_100/experiment1_100.raw.kwd\n",
      "2016-09-14_10-09-22_100\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_10-43-25_100/experiment1_100.raw.kwd\n",
      "2016-09-14_10-43-25_100\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_12-00-34_100/experiment1_100.raw.kwd\n",
      "2016-09-14_12-00-34_100\n",
      "File /usr/local/experiment/raw_data/z007/2016-09-14_15-26-11_100/experiment1_100.raw.kwd\n",
      "2016-09-14_15-26-11_100\n",
      "cube backup: 2016-09-14_12-00-34_100\n",
      "passaro Data backup: 2016-09-14_12-00-34_100\n",
      "cube backup: 2016-09-14_10-06-02_100\n",
      "passaro Data backup: 2016-09-14_10-06-02_100\n",
      "cube backup: 2016-09-14_09-41-13_100\n",
      "passaro Data backup: 2016-09-14_09-41-13_100\n",
      "cube backup: 2016-09-14_10-09-22_100\n",
      "passaro Data backup: 2016-09-14_10-09-22_100\n",
      "cube backup: 2016-09-14_10-43-25_100\n",
      "passaro Data backup: 2016-09-14_10-43-25_100\n",
      "cube backup: 2016-09-14_15-26-11_100\n",
      "passaro Data backup: 2016-09-14_15-26-11_100\n",
      "[u'0', u'1', u'2', u'3', u'4', u'5']\n",
      "0\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_000.mic.wav\n",
      "1\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_001.mic.wav\n",
      "2\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_002.mic.wav\n",
      "3\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_003.mic.wav\n",
      "4\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_004.mic.wav\n",
      "5\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-2016-09-14/experiment-rec_005.mic.wav\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "bird_id = 'z007'\n",
    "sess_day = '2016-09-14'\n",
    "depth = '100'\n",
    "\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "raw_data_folder = et.file_names(bird_id)['folders']['raw']\n",
    "sessions = glob.glob(os.path.join(raw_data_folder, sess_day + '*' + str(depth)))\n",
    "day_par_file_path = os.path.join(sessions[0], et.file_names(bird_id)['files']['par'])\n",
    "sess_par = et.get_parameters(bird_id, os.path.split(sessions[0])[-1], location='raw')\n",
    "super_sess_par = sess_par.copy()\n",
    "data_processor = sess_par['rec_config']['processors']['data']\n",
    "experiments = list_flatten([glob.glob(os.path.join(s, '*_{}.raw.kwd'.format(data_processor)))[:] for s in sessions ])\n",
    "experiments.sort()\n",
    "\n",
    "# make the supersession file\n",
    "super_sess_name = 'day-' + sess_day\n",
    "fn = et.file_names(bird_id, super_sess_name, 0)\n",
    "super_sess_path = fn['folders']['ss']\n",
    "super_file_path = os.path.join(super_sess_path, fn['files']['ss_raw'])\n",
    "\n",
    "print \"Making supersession {}\".format(super_sess_name)\n",
    "print super_file_path\n",
    "mkdir_p(super_sess_path)\n",
    "\n",
    "print \"making the superfile\"\n",
    "make_super_file(super_file_path)\n",
    "with h5py.File(super_file_path, 'a') as super_file:\n",
    "    for experiment_path in experiments:\n",
    "        print 'File {0}'.format(experiment_path)\n",
    "        sess_fold = os.path.split(os.path.split(experiment_path)[0])[1]\n",
    "        print sess_fold\n",
    "        sess_par = et.get_parameters(bird_id, sess_fold, location='raw')\n",
    "        kwd_chan_list, new_par_chan_config = new_channel_config(sess_par['channel_config'])\n",
    "        with h5py.File(experiment_path, 'r') as raw_file:\n",
    "            insert_experiment_groups(super_file, raw_file, kwd_chan_list)\n",
    "        super_file.flush()\n",
    "\n",
    "super_sess_par['channel_config'] =  new_par_chan_config.copy()\n",
    "save_ss_par(super_sess_par, bird_id, super_sess_name)\n",
    "\n",
    "\n",
    "#make the backups\n",
    "bkp_dest_path = os.path.join(cube_experiment_folder, 'raw_data', bird_id)\n",
    "store_dest_path = os.path.join(store_experiment_folder, 'raw_data', bird_id)\n",
    "\n",
    "mkdir_p(bkp_dest_path)\n",
    "mkdir_p(store_dest_path)\n",
    "for session_path in sessions:\n",
    "    session_name = os.path.split(session_path)[-1]\n",
    "    print \"cube backup: \" + session_name\n",
    "    dest_bkp = os.path.join(bkp_dest_path, session_name)\n",
    "    try:\n",
    "        sh.copytree(session_path, dest_bkp)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == 17:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "    print \"passaro Data backup: \" + session_name\n",
    "    sh.move(session_path, store_dest_path)\n",
    "    \n",
    "extract_wav_chans(bird_id, super_sess_name)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/experiment/raw_data/z027/z027/no-neural_2016-08-30*'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(raw_data_folder, bird_id, sess_day + '*' + str(depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make one single binary file\n",
    "bird = 'z020'\n",
    "sess = 'day-2016-06-05'\n",
    "fn = et.file_names(bird, sess)\n",
    "params = et.get_parameters(bird, sess)\n",
    "kwd_path = et.file_path(fn, 'ss', 'ss_raw')\n",
    "bin_path = et.file_path(fn, 'ss', 'ss_bin')\n",
    "chan_list = params['channel_config']['neural']\n",
    "h5.kwd_to_binary(kwd_path, bin_path, chan_list=chan_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bird_id = 'z017'\n",
    "sess_day = '2016-07-04'\n",
    "depth = 300\n",
    "\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "fn = et.file_names(bird_id)\n",
    "sessions = glob.glob(os.path.join(fn['folders']['raw'], sess_day + '*' + str(depth)))\n",
    "experiments = list_flatten([ glob.glob(os.path.join(s, '*.raw.kwd'))[:] for s in sessions ])\n",
    "experiments.sort()\n",
    "\n",
    "# make the supersession file\n",
    "super_sess_name = 'day-' + sess_day\n",
    "fn = et.file_names(bird_id, super_sess_name, 0)\n",
    "super_sess_path = fn['folders']['ss']\n",
    "super_file_path = os.path.join(super_sess_path, fn['files']['ss_raw'])\n",
    "#bin_file_path = os.path.join(super_sess_path, fn['files']['ss_bin'])\n",
    "\n",
    "super_file = et.open_kwd(bird_id, super_sess_name)\n",
    "\n",
    "bin_file_path = os.path.join(os.path.split(super_file_path)[0],'experiment.raw.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making supersession day-no-neural_2016-09-01\n",
      "/mnt/cube/earneodo/bci_zf/ss_data/z007/day-no-neural_2016-09-01/experiment.raw.kwd\n",
      "Copying parameter file: \n"
     ]
    }
   ],
   "source": [
    "bird_id = 'z007'\n",
    "sess_day = 'no-neural_2016-09-01'\n",
    "depth = ''\n",
    "\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "raw_data_folder = et.file_names(bird_id)['folders']['raw']\n",
    "sessions = glob.glob(os.path.join(raw_data_folder, sess_day + '*' + str(depth)))\n",
    "day_par_file_path = os.path.join(sessions[0], fn['files']['par'])\n",
    "sess_par = et.get_parameters(bird_id, os.path.split(sessions[0])[-1], location='raw')\n",
    "super_sess_par = sess_par.copy()\n",
    "data_processor = sess_par['rec_config']['processors']['data']\n",
    "experiments = list_flatten([glob.glob(os.path.join(s, '*_{}.raw.kwd'.format(data_processor)))[:] for s in sessions ])\n",
    "experiments.sort()\n",
    "\n",
    "# make the supersession file\n",
    "super_sess_name = 'day-' + sess_day\n",
    "fn = et.file_names(bird_id, super_sess_name, 0)\n",
    "super_sess_path = fn['folders']['ss']\n",
    "super_file_path = os.path.join(super_sess_path, fn['files']['ss_raw'])\n",
    "\n",
    "print \"Making supersession {}\".format(super_sess_name)\n",
    "print super_file_path\n",
    "mkdir_p(super_sess_path)\n",
    "\n",
    "print \"making the superfile\"\n",
    "make_super_file(super_file_path)\n",
    "with h5py.File(super_file_path, 'a') as super_file:\n",
    "    for experiment_path in experiments:\n",
    "        print 'File {0}'.format(experiment_path)\n",
    "        sess_fold = os.path.split(os.path.split(experiment_path)[0])[1]\n",
    "        print sess_fold\n",
    "        sess_par = et.get_parameters(bird_id, sess_fold, location='raw')\n",
    "        kwd_chan_list, new_par_chan_config = new_channel_config(sess_par['channel_config'])\n",
    "        with h5py.File(experiment_path, 'r') as raw_file:\n",
    "            insert_experiment_groups(super_file, raw_file, kwd_chan_list)\n",
    "        super_file.flush()\n",
    "\n",
    "super_sess_par['channel_config'] =  new_par_chan_config.copy()\n",
    "save_ss_par(super_sess_par, bird_id, super_sess_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwd_chan_list, new_par_chan_list = new_channel_config(sess_par['channel_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 35, 36, 37]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ss_par = sess_par.copy()\n",
    "kwd_chan_list, ss_chan_config = new_channel_config(sess_par['channel_config'])\n",
    "ss_par['channel_config'] =  ss_chan_config.copy()\n",
    "save_ss_par(ss_par, bird_id, super_sess_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
