{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.signal as sg\n",
    "import math\n",
    "import scipy as sp\n",
    "import pylab\n",
    "import os\n",
    "import wave\n",
    "import scipy.io.wavfile\n",
    "import h5py\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bird_id = 'z020'\n",
    "rawdata_folder = os.path.abspath('/Users/Kai/Documents/UCSD/Research/Gentner group/Raw Data')\n",
    "save_file = os.path.join(rawdata_folder, bird_id, 'motif_set.p')\n",
    "datasets = pickle.load(open(save_file, 'rb'))\n",
    "\n",
    "#find maximum of all datasets and find out how many digits we need to store binary values\n",
    "maxnum = 0\n",
    "minnum = 0\n",
    "for i in range(len(datasets)):\n",
    "    if max(datasets[i]) > maxnum:\n",
    "        maxnum = max(datasets[i])\n",
    "    if min(datasets[i]) < minnum:\n",
    "        minnum = min(datasets[i])\n",
    "min_scaled = minnum//200\n",
    "max_scaled = maxnum//200\n",
    "print(minnum, min_scaled, maxnum, max_scaled)\n",
    "mag_size = max_scaled-min_scaled+1\n",
    "print(mag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training sets and validation sets\n",
    "train_sets = datasets[10:]\n",
    "valid_sets = datasets[:10]\n",
    "\n",
    "#function d2blist returns a list that contains all the 1 indexings of dec's binary form\n",
    "def num2index(num):\n",
    "    index = num//200-min_scaled\n",
    "    return index\n",
    "\n",
    "def index2num(index):\n",
    "    num = (index+min_scaled)*200+random.randint(0, 100)\n",
    "    return num\n",
    "\n",
    "def magnitude(probabilities):\n",
    "    return [index2num(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "batch_size_per_set = 32\n",
    "num_unrollings = 100\n",
    "train_set_size = len(train_sets)\n",
    "valid_set_size = len(valid_sets)\n",
    "\n",
    "print(num2index(-12482),index2num(num2index(-12482)))\n",
    "print(len(train_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate batches for training every time\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, song, batch_size, num_unrollings):\n",
    "    self._song = song\n",
    "    self._song_size = len(song)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._song_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, mag_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, num2index(self._song[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._song_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "  \n",
    "def batches2num(batches):\n",
    "    numlist = list()\n",
    "    for i in range(batches[0].shape[0]):\n",
    "        batch = list()\n",
    "        for j in range(len(batches)):\n",
    "            thisnum = batches[j][i,:]\n",
    "            index = thisnum.tolist().index(1.0)\n",
    "            num = index2num(index)\n",
    "            batch.append(num)\n",
    "        numlist.append(batch)\n",
    "    return numlist\n",
    "\n",
    "train_batches = BatchGenerator(train_sets[0], batch_size_per_set, num_unrollings)\n",
    "\n",
    "thisbatch = batches2num(train_batches.next())\n",
    "print(thisbatch[0])\n",
    "plt.figure(1)\n",
    "plt.plot(thisbatch[0])\n",
    "pylab.show()\n",
    "#print(batches2num(train_batches.next()))\n",
    "#print(batches2num(train_batches.next()))\n",
    "#print(batches2num(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    p = np.zeros(shape=[1, mag_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, mag_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([mag_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([mag_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([mag_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([mag_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    #saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    #saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size_per_set, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size_per_set, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, mag_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([mag_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size_per_set,mag_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, mag_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        train_batches = BatchGenerator(train_sets[step%train_set_size], batch_size_per_set, num_unrollings)\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 4) == 0:\n",
    "            # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                song = list()\n",
    "                feed = sample(random_distribution())\n",
    "                song.append(magnitude(feed)[0])\n",
    "                reset_sample_state.run()\n",
    "                for _ in range(4000):\n",
    "                    prediction = sample_prediction.eval({sample_input: feed})\n",
    "                    feed = sample(prediction)\n",
    "                    song.append(magnitude(feed)[0])\n",
    "                plt.plot(song)\n",
    "                plt.xlabel('Time')\n",
    "                plt.ylabel('Amplitude')\n",
    "                pylab.show()\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_set_size):\n",
    "                valid_batches = BatchGenerator(valid_sets[step%valid_set_size], 1, 1)\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_set_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
