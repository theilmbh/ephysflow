{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer: passaro\n"
     ]
    }
   ],
   "source": [
    "# tampering with the kwd file on passaro\n",
    "# the /experiment folder is on the ssd of passaro (/usr/local/experiment/raw_data)\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pdb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.signal as sg\n",
    "import math\n",
    "import scipy as sp\n",
    "import socket\n",
    "import os\n",
    "import wave\n",
    "import struct\n",
    "import h5py\n",
    "from scipy.signal import hilbert\n",
    "from basic_viewing import h5_functions as h5\n",
    "import glob\n",
    "import errno    \n",
    "import os\n",
    "import shutil as sh\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "# Check wihic computer to decide where the things are mounted\n",
    "comp_name=socket.gethostname()\n",
    "print 'Computer: ' + comp_name\n",
    "if  comp_name == 'chim':\n",
    "    #sys.path.append('/Users/zeke/experiment/ephysDataManagement/ephysScripts')\n",
    "    experiment_folder = os.path.join(os.path.abspath('W:') , \"earneodo\", \"bci_zf\")\n",
    "elif  'lookfar' in comp_name:\n",
    "    repos_folder = os.path.abspath('/Users/zeke/reposs')\n",
    "    experiment_folder = os.path.join(os.path.abspath('/Volumes') , 'gentner', 'earneodo', 'bci_zf')\n",
    "\n",
    "elif 'lintu' in comp_name:\n",
    "    repos_folder = os.path.abspath('/mnt/cube/earneodo/repos')\n",
    "    experiment_folder = os.path.join('/mnt/cube/earneodo/bci_zf/')\n",
    "\n",
    "elif 'passaro' in comp_name:\n",
    "    repos_folder = os.path.abspath('/mnt/cube/earneodo/repos')\n",
    "    cube_experiment_folder = os.path.join('/mnt/cube/earneodo/bci_zf/')\n",
    "    store_experiment_folder = os.path.join('/Data/bci_zf/')\n",
    "    experiment_folder = os.path.join('/usr/local/experiment')\n",
    "\n",
    "sys.path.append(os.path.join(repos_folder, 'soundflow', 'sound_tools'))\n",
    "sys.path.append(os.path.join(repos_folder, 'ephysflow'))\n",
    "sys.path.append(os.path.join(repos_folder, 'analysis-tools'))\n",
    "\n",
    "import soundtools as st\n",
    "import Kwik as oe\n",
    "from scipy.io import wavfile\n",
    "\n",
    "raw_data_folder = os.path.join(experiment_folder, 'raw_data')\n",
    "ss_data_folder = os.path.join(cube_experiment_folder, 'ss_data')\n",
    "\n",
    "def create_neural_data_set(data_set, parent_group, channel_list, frame_size=None, processing=None, *args, **kwargs):\n",
    "    # make the new dataset\n",
    "    d_type = data_set.dtype\n",
    "    nd_cols = np.array(channel_list).size\n",
    "    nd_rows = data_set.shape[0]\n",
    "    d_chunks = np.array(data_set.chunks)\n",
    "    \n",
    "    if d_chunks[0] > nd_rows:\n",
    "        d_chunks[0] = nd_rows\n",
    "    \n",
    "    if frame_size is None:\n",
    "        frame_size = d_chunks[0] if d_chunks[0]<nd_rows else nd_rows\n",
    "    \n",
    "    neural_dset = parent_group.create_dataset(\"data\", (nd_rows, nd_cols), \n",
    "                                              chunks=(d_chunks[0], nd_cols), dtype=d_type)\n",
    "    \n",
    "    copy_frame = np.zeros([frame_size, nd_cols], dtype=d_type)\n",
    "    n_full_frames = int(data_set.shape[0]/frame_size)\n",
    "    #print neural_dset.shape\n",
    "    # Fill the dataset:\n",
    "    for i in range(n_full_frames):\n",
    "        copy_frame = h5.load_table_slice(data_set, np.arange(i*frame_size, (i+1)*frame_size) , channel_list)\n",
    "        #print copy_frame.shape\n",
    "        #neural_dset.write_direct(copy_frame, dest_sel=np.s_[i*frame_size: (i+1)*frame_size, 0:nd_cols])\n",
    "        neural_dset[i*frame_size: (i+1)*frame_size, 0:nd_cols] = copy_frame\n",
    "    \n",
    "    if frame_size*n_full_frames < nd_rows:\n",
    "        last_frame_size = nd_rows - frame_size*n_full_frames\n",
    "        copy_frame = np.zeros([last_frame_size, nd_cols], dtype=d_type)\n",
    "        copy_frame = h5.load_table_slice(data_set, \n",
    "                                         np.arange(n_full_frames*frame_size, \n",
    "                                                   n_full_frames*frame_size + last_frame_size),\n",
    "                                         channel_list)\n",
    "    # copy the attributes:\n",
    "    neural_dset.attrs.create('valid_samples', np.array(data_set.attrs['valid_samples'][channel_list]))\n",
    "    \n",
    "def copy_attribs(source, dest):\n",
    "    for key, attrib in source.attrs.iteritems():\n",
    "        dest.attrs.create(key, attrib, dtype=attrib.dtype)\n",
    "        \n",
    "def copy_application_data(source_rec, dest_rec, chan_list,\n",
    "                          resize_keys = ['channel_bit_volts', 'channel_sample_rates']):\n",
    "    \n",
    "    dest_rec.create_group('application_data')\n",
    "    for key, attrib in source_rec['application_data'].attrs.iteritems():\n",
    "        if not key in resize_keys:\n",
    "            dest_rec['application_data'].attrs.create(key, attrib, dtype=attrib.dtype)\n",
    "        else:\n",
    "            #print key\n",
    "            #print attrib[chan_list]\n",
    "            dest_rec['application_data'].attrs.create(key, attrib[chan_list], dtype=attrib.dtype)\n",
    "\n",
    "def check_rec_data(source_rec):\n",
    "    #check the data table of a rec group\n",
    "    data_set = source_rec['data']\n",
    "    valid_samples = None\n",
    "    try:\n",
    "        valid_samples = data_set.attrs.get('valid_samples')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    has_data = True if valid_samples is not None and valid_samples[0]>0 else False\n",
    "    return has_data\n",
    "\n",
    "def get_valid_samples(source_rec):\n",
    "    #get the valid samples declared by the data table attributes\n",
    "    data_set = source_rec['data']\n",
    "    valid_samples = None\n",
    "    try:\n",
    "        valid_samples = data_set.attrs.get('valid_samples')[0]\n",
    "    except:\n",
    "        pass\n",
    "    return valid_samples\n",
    "\n",
    "def create_data_groups(raw_file, new_file, chan_list):\n",
    "    for rec_name, rec_group in raw_file['/recordings'].iteritems():\n",
    "        new_file['/recordings'].create_group(rec_name)\n",
    "        copy_attribs(rec_group, new_file['/recordings'][rec_name])\n",
    "        copy_application_data(rec_group, new_file['/recordings'][rec_name], chan_list)\n",
    "        create_neural_data_set(rec_group['data'], new_file['/recordings'][rec_name], chan_list)\n",
    "        \n",
    "def make_shank_kwd(raw_file, out_file_path, chan_list):\n",
    "    ss_file = h5py.File(out_file_path, 'w')\n",
    "    copy_attribs(raw_file, ss_file)\n",
    "    ss_file.create_group('/recordings')\n",
    "    create_data_groups(raw_file, ss_file, chan_list) \n",
    "    ss_file.close()\n",
    "    \n",
    "def list_flatten(x):\n",
    "    return [val for sublist in x for val in sublist]\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def make_super_file(path):\n",
    "    new_file = h5py.File(path, 'w')\n",
    "    new_file.create_group('/recordings')\n",
    "    new_file.close()\n",
    "\n",
    "def create_data_group(raw_file, new_file, chan_list, new_rec_name):\n",
    "    for rec_name, rec_group in raw_file['/recordings'].iteritems():\n",
    "        new_file['/recordings'].create_group(rec_name)\n",
    "        copy_attribs(rec_group, new_file['/recordings'][rec_name])\n",
    "        copy_application_data(rec_group, new_file['/recordings'][rec_name], chan_list)\n",
    "        create_neural_data_set(rec_group['data'], new_file['/recordings'][rec_name], chan_list)\n",
    "\n",
    "def insert_neural_rec_group(dest_file, raw_rec_group, chan_list, new_group_name=None):\n",
    "    rec_name = raw_rec_group.name if new_group_name is None else new_group_name\n",
    "    \n",
    "    dest_file['/recordings'].create_group(rec_name)\n",
    "    new_rec = dest_file['/recordings'][rec_name]\n",
    "    copy_application_data(raw_rec_group, new_rec, chan_list)\n",
    "    copy_attribs(raw_rec_group, new_rec)\n",
    "    create_neural_data_set(raw_rec_group['data'], new_rec, chan_list)\n",
    "    \n",
    "def modify_rec_group_attribs(kwd_file, rec_name, attr_dict, new_attr_dict = None):\n",
    "    for key, value in attr_dict.iteritems():\n",
    "        kwd_file['/recordings'][rec_name].attrs.modify(key, value)\n",
    "    \n",
    "    if new_attr_dict is not None:\n",
    "        for key, value in new_attr_dict.iteritems():\n",
    "            kwd_file['/recordings'][rec_name].attrs.create(key, value)\n",
    "\n",
    "def get_recs_list(experiment_file):\n",
    "    if experiment_file['/recordings'].keys() == []:\n",
    "        rec_list = None\n",
    "    else:\n",
    "        rec_list = np.array([int(i) for i in experiment_file['/recordings'].keys()])\n",
    "    return rec_list\n",
    "\n",
    "def get_experiment_endpoints(experiment_file):\n",
    "    recs_list = get_recs_list(experiment_file)\n",
    "    if recs_list is not None:\n",
    "        last_rec_num = np.max(recs_list)\n",
    "        last_rec_name = str(last_rec_num)\n",
    "        last_rec = experiment_file['/recordings'][last_rec_name]\n",
    "        last_rec_start = last_rec.attrs['start_sample']\n",
    "        last_rec_nsamples = last_rec['data'].shape[0]\n",
    "        next_rec_num = last_rec_num + 1\n",
    "        next_sample = last_rec_start + last_rec_nsamples\n",
    "    else:\n",
    "        next_rec_num = 0\n",
    "        next_sample = 0\n",
    "        \n",
    "    return [next_rec_num, next_sample]\n",
    "            \n",
    "\n",
    "def insert_experiment_groups(dest_file, raw_file, chan_list):\n",
    "    # all the recs in an experiment file if they pass check\n",
    "    for raw_rec_name, raw_rec_group in raw_file['/recordings'].iteritems():\n",
    "        #print 'rec {0}'.format(raw_rec_name)\n",
    "        s_f = raw_rec_group.attrs['sample_rate']\n",
    "        if check_rec_data(raw_rec_group):\n",
    "            target_rec_num, target_start_sample = get_experiment_endpoints(dest_file)\n",
    "            target_rec_name = str(target_rec_num)\n",
    "            target_start_time = int(target_start_sample/(0.001*s_f))\n",
    "            target_source = '{0}:''/''recordings/{1}'.format(raw_file.filename, raw_rec_name)\n",
    "            insert_neural_rec_group(dest_file, raw_rec_group, chan_list, new_group_name=target_rec_name)\n",
    "            modify_rec_group_attribs(dest_file, target_rec_name, \n",
    "                             {'start_sample' : target_start_sample},\n",
    "                              new_attr_dict = {'name' : target_source,\n",
    "                                               'start_time' : target_start_time})\n",
    "        else:\n",
    "            #print \"Skipping rec {0} with no data\".format(raw_rec_name)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bird_id = 'z017'\n",
    "sess_day = '2016-06-20'\n",
    "depth = '150'\n",
    "\n",
    "channel_config = {'neural': [0, 11, 9, 16, 14, 5, 8, 10, 15, 2, 7, 4, 3, 5, 1, 16, 15],\n",
    "                  'super' : range(17),\n",
    "                  '0' : [11, 15, 8, 10],\n",
    "                  '1' : [14, 9, 13, 12],\n",
    "                  '2' : [2, 3, 1, 6],\n",
    "                  '3' : [5, 7, 0, 4], \n",
    "                  'test' : [11],\n",
    "                 'sound': [17]}\n",
    "\n",
    "chan_list= channel_config['super']\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "sessions = glob.glob(os.path.join(raw_data_folder, bird_id, sess_day + '*'))\n",
    "experiments = list_flatten([ glob.glob(os.path.join(s, '*.raw.kwd'))[:] for s in sessions ])\n",
    "experiments.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bird_id = 'z017'\n",
    "sess_day = '2016-06-20'\n",
    "depth = 225\n",
    "\n",
    "channel_config = {'neural': [0, 11, 9, 16, 14, 5, 8, 10, 15, 2, 7, 4, 3, 5, 1, 16, 15],\n",
    "                  'super' : range(17),\n",
    "                  '0' : [11, 15, 8, 10],\n",
    "                  '1' : [14, 9, 13, 12],\n",
    "                  '2' : [2, 3, 1, 6],\n",
    "                  '3' : [5, 7, 0, 4], \n",
    "                  'test' : [11],\n",
    "                 'sound': [17]}\n",
    "\n",
    "chan_list= channel_config['super']\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "sessions = glob.glob(os.path.join(raw_data_folder, bird_id, sess_day + '*' + str(depth)))\n",
    "experiments = list_flatten([ glob.glob(os.path.join(s, '*.raw.kwd'))[:] for s in sessions ])\n",
    "experiments.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/experiment/raw_data/z017/2016-06-20_08-55-34_225']\n"
     ]
    }
   ],
   "source": [
    "sessions = glob.glob(os.path.join(raw_data_folder, bird_id, sess_day + '*' + str(depth)))\n",
    "print sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cube/earneodo/bci_zf/ss_data/z017/day-2016-07-07/experiment.raw.kwd\n",
      "File /usr/local/experiment/raw_data/z017/2016-07-07_09-52-29_225/experiment1_100.raw.kwd\n",
      "File /usr/local/experiment/raw_data/z017/2016-07-07_14-37-35_225/experiment1_100.raw.kwd\n",
      "cube backup: 2016-07-07_14-37-35_225\n",
      "passaro Data backup: 2016-07-07_14-37-35_225\n",
      "cube backup: 2016-07-07_09-52-29_225\n",
      "passaro Data backup: 2016-07-07_09-52-29_225\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "bird_id = 'z017'\n",
    "sess_day = '2016-07-07'\n",
    "depth = 225\n",
    "\n",
    "channel_config = {'neural': [0, 11, 9, 16, 14, 5, 8, 10, 15, 2, 7, 4, 3, 5, 1, 16, 15],\n",
    "                  'super' : range(17),\n",
    "                  '0' : [11, 15, 8, 10],\n",
    "                  '1' : [14, 9, 13, 12],\n",
    "                  '2' : [2, 3, 1, 6],\n",
    "                  '3' : [5, 7, 0, 4], \n",
    "                  'test' : [11],\n",
    "                 'sound': [17]}\n",
    "\n",
    "chan_list= channel_config['super']\n",
    "# Make the super session and gather the list of sessions\n",
    "# get the sessions list of files\n",
    "sessions = glob.glob(os.path.join(raw_data_folder, bird_id, sess_day + '*' + str(depth)))\n",
    "experiments = list_flatten([ glob.glob(os.path.join(s, '*.raw.kwd'))[:] for s in sessions ])\n",
    "experiments.sort()\n",
    "\n",
    "# make the supersession file\n",
    "super_sess_path = os.path.join(ss_data_folder, bird_id, 'day-' + sess_day)\n",
    "mkdir_p(super_sess_path)\n",
    "super_file_path = os.path.join(super_sess_path, 'experiment.raw.kwd')\n",
    "print super_file_path\n",
    "make_super_file(super_file_path)\n",
    "\n",
    "super_file = h5py.File(super_file_path, 'a')\n",
    "for experiment_path in experiments:\n",
    "    print 'File {0}'.format(experiment_path)\n",
    "    raw_file = h5py.File(experiment_path, 'r')\n",
    "    insert_experiment_groups(super_file, raw_file, chan_list)\n",
    "    raw_file.close()\n",
    "    super_file.flush()\n",
    "\n",
    "super_file.close()\n",
    "\n",
    "#make the backups\n",
    "bkp_dest_path = os.path.join(cube_experiment_folder, 'raw_data', bird_id)\n",
    "store_dest_path = os.path.join(store_experiment_folder, 'raw_data', bird_id)\n",
    "\n",
    "mkdir_p(bkp_dest_path)\n",
    "mkdir_p(store_dest_path)\n",
    "for session_path in sessions:\n",
    "    session_name = os.path.split(session_path)[-1]\n",
    "    print \"cube backup: \" + session_name\n",
    "    dest_bkp = os.path.join(bkp_dest_path, session_name)\n",
    "    try:\n",
    "        sh.copytree(session_path, dest_bkp)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == 17:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "    print \"passaro Data backup: \" + session_name\n",
    "    sh.move(session_path, store_dest_path)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_method = 'pandas'\n",
    "rolling_method is 'pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
